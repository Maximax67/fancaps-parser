{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fancaps-parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q beautifulsoup4 tqdm httpx[http2]==0.24.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import httpx\n",
    "import random\n",
    "\n",
    "import concurrent.futures\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './'\n",
    "\n",
    "movies_list_filename = 'movies_list.csv'\n",
    "tv_list_filename = 'tv_list.csv'\n",
    "anime_list_filename = 'anime_list.csv'\n",
    "\n",
    "movies_data_filename = 'movies_data.json'\n",
    "tv_data_filename = 'tv_data.json'\n",
    "anime_data_filename = 'anime_data.json'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "movies_list_filepath = os.path.join(output_dir, movies_list_filename)\n",
    "tv_list_filepath = os.path.join(output_dir, tv_list_filename)\n",
    "anime_list_filepath = os.path.join(output_dir, anime_list_filename)\n",
    "\n",
    "movies_data_filepath = os.path.join(output_dir, movies_data_filename)\n",
    "tv_data_filepath = os.path.join(output_dir, tv_data_filename)\n",
    "anime_data_filepath = os.path.join(output_dir, anime_data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bypass Cloudflare settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = '...' # Paste you user-agent header\n",
    "cf_clearance = '...' # Paste cf_clearance token\n",
    "\n",
    "headers = {\n",
    "    'Host': 'fancaps.net',\n",
    "    'User-Agent': user_agent,\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Accept-Encoding': 'gzip, deflate, br, zstd'\n",
    "}\n",
    "\n",
    "cookies = {\n",
    "    'cf_clearance': cf_clearance\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_title_and_year(movie_title: str):\n",
    "    match = re.match(r\"^(.*)\\((\\d{4})\\)$\", movie_title.strip())\n",
    "    if match:\n",
    "        title = match.group(1).strip()\n",
    "        year = int(match.group(2))\n",
    "        return title, year\n",
    "\n",
    "    return movie_title, None\n",
    "\n",
    "\n",
    "def split_title_and_season(tv_serial_title: str):\n",
    "    match = re.match(r\"^(.*) Season (\\d+)$\", tv_serial_title.strip())\n",
    "    if match:\n",
    "        title = match.group(1).strip()\n",
    "        season = int(match.group(2))\n",
    "        return title, season\n",
    "    else:\n",
    "        return tv_serial_title, None\n",
    "\n",
    "\n",
    "def parse_id_in_url(url: str):\n",
    "    return int(url.split('?', 1)[1].split('-', 1)[0])\n",
    "\n",
    "\n",
    "def get_fancaps_list_items(url, headers, cookies):\n",
    "    response = httpx.get(url, headers=headers, cookies=cookies, timeout=60)\n",
    "\n",
    "    status_code = response.status_code\n",
    "    if status_code != 200:\n",
    "        print(f'Failed to fetch the website! Status code: {status_code}')\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    results_content = soup.find('div', class_='single_post_content')\n",
    "    rows = results_content.find_all('div', class_='row')\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "def parse_fancaps_movies_list(url, headers, cookies):\n",
    "    results = []\n",
    "    rows = get_fancaps_list_items(url, headers, cookies)\n",
    "    if rows is None:\n",
    "        return results\n",
    "\n",
    "    for row in rows:\n",
    "        h4_tag = row.find('h4')\n",
    "        if not h4_tag:\n",
    "            continue # Skip add block\n",
    "\n",
    "        a_tag = h4_tag.find('a')\n",
    "        title_parsed = a_tag.text.strip()[:-7] # Remove ' Images'\n",
    "\n",
    "        parsed_url = a_tag['href']\n",
    "        title, year = split_title_and_year(title_parsed)\n",
    "        id = int(parsed_url.partition('&movieid=')[2])\n",
    "\n",
    "        results.append((title, year, id))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def parse_fancaps_tv_list(url, headers, cookies):\n",
    "    results = []\n",
    "    rows = get_fancaps_list_items(url, headers, cookies)\n",
    "    if rows is None:\n",
    "        return results\n",
    "\n",
    "    for row in rows:\n",
    "        h4_tag = row.find('h4')\n",
    "        if not h4_tag:\n",
    "            continue # Skip add block\n",
    "\n",
    "        a_tag = h4_tag.find('a')\n",
    "        title_parsed = a_tag.text.strip()\n",
    "\n",
    "        parsed_url = a_tag['href']\n",
    "        title, season = split_title_and_season(title_parsed)\n",
    "        id = parse_id_in_url(parsed_url)\n",
    "\n",
    "        results.append((title, season, id))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def parse_fancaps_anime_list(url, headers, cookies):\n",
    "    results = []\n",
    "    rows = get_fancaps_list_items(url, headers, cookies)\n",
    "    if rows is None:\n",
    "        return results\n",
    "\n",
    "    for row in rows:\n",
    "        h4_tag = row.find('h4')\n",
    "        if not h4_tag:\n",
    "            continue # Skip add block\n",
    "\n",
    "        a_tag = h4_tag.find('a')\n",
    "        title = a_tag.text.strip()\n",
    "\n",
    "        span = row.find('span')\n",
    "        alternative_title = span.text.strip()\n",
    "\n",
    "        parsed_url = a_tag['href']\n",
    "        id = parse_id_in_url(parsed_url)\n",
    "\n",
    "        results.append((title, alternative_title, id))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def dump_fancaps_lists_to_csv(results, filename, header):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(header)\n",
    "        for result in results:\n",
    "            writer.writerow(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse movie list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_list_url = 'https://fancaps.net/movies/showList.php?%'\n",
    "movies_list = parse_fancaps_movies_list(movies_list_url, headers, cookies)\n",
    "\n",
    "print('Total parsed:', len(movies_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_list_csv_headers = ['Title', 'Year', 'Id']\n",
    "\n",
    "dump_fancaps_lists_to_csv(movies_list, movies_list_filepath, movies_list_csv_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse tv list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_list_url = 'https://fancaps.net/tv/showList.php?%'\n",
    "tv_list = parse_fancaps_tv_list(tv_list_url, headers, cookies)\n",
    "\n",
    "print('Total parsed:', len(tv_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_list_csv_headers = ['Title', 'Season', 'Id']\n",
    "\n",
    "dump_fancaps_lists_to_csv(tv_list, tv_list_filepath, tv_list_csv_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse anime list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_list_url = 'https://fancaps.net/anime/showList.php?%'\n",
    "anime_list = parse_fancaps_anime_list(anime_list_url, headers, cookies)\n",
    "\n",
    "print('Total parsed:', len(anime_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_list_csv_headers = ['Title', 'Alternative title', 'Id']\n",
    "\n",
    "dump_fancaps_lists_to_csv(anime_list, anime_list_filepath, anime_list_csv_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_id(url: str) -> int:\n",
    "    last_dot_pos = url.rfind('.')\n",
    "    last_slash_pos = url.rfind('/', 0, last_dot_pos-1)\n",
    "\n",
    "    image_id_str = url[last_slash_pos + 1:last_dot_pos]\n",
    "    image_id = int(image_id_str)\n",
    "\n",
    "    return image_id\n",
    "\n",
    "\n",
    "def fetch_url_with_retries(url, headers, cookies, retries=3):\n",
    "    for i in range(1, retries + 1):\n",
    "        try:\n",
    "            response = httpx.get(url, headers=headers, cookies=cookies, follow_redirects=True)\n",
    "            status_code = response.status_code\n",
    "\n",
    "            if status_code == 200:\n",
    "                if i != 1:\n",
    "                    print(f'URL fetched successfully after {i-1} retr{\"y\" if i == 2 else \"ies\"}: {url}')\n",
    "\n",
    "                return response.content\n",
    "\n",
    "            print(f'Failed to fetch the website! Status code: {status_code}. URL: {url}. Retrying {i}\\{retries}...')\n",
    "            time.sleep(1 + random.random())\n",
    "        except Exception as e:\n",
    "            print(f'Exception while fetching by url ({url}): {e}. Retrying {i}\\{retries}...')\n",
    "            time.sleep(1 + random.random())\n",
    "\n",
    "    raise Exception(f'Number of retries ({retries}) exceeded for url: {url}')\n",
    "\n",
    "\n",
    "def fetch_movie_with_retries(title, year, movie_id, headers, cookies, retries=3):\n",
    "    url = 'https://fancaps.net/movies/MovieImages.php?movieid=' + str(movie_id)\n",
    "    response_content = fetch_url_with_retries(url, headers, cookies, retries)\n",
    "\n",
    "    soup = BeautifulSoup(response_content, 'html.parser')\n",
    "    images_bar = soup.find('section', {'id': 'contentbody'}).find('div', {'class': 'middle_bar'})\n",
    "\n",
    "    first_image = images_bar.find('img')\n",
    "    first_image_src = first_image['src']\n",
    "    first_image_id = extract_image_id(first_image_src)\n",
    "\n",
    "    next_url = url + '&page=999999'\n",
    "    response_content = fetch_url_with_retries(next_url, headers, cookies, retries)\n",
    "\n",
    "    soup = BeautifulSoup(response_content, 'html.parser')\n",
    "    images_bar = soup.find('section', {'id': 'contentbody'}).find('div', {'class': 'middle_bar'})\n",
    "\n",
    "    last_image = images_bar.find_all('img')[-1]\n",
    "    last_image_src = last_image['src']\n",
    "    last_image_id = extract_image_id(last_image_src)\n",
    "\n",
    "    return {\n",
    "        'id': movie_id,\n",
    "        'title': title,\n",
    "        'year': year,\n",
    "        'first_image_id': first_image_id,\n",
    "        'last_image_id': last_image_id\n",
    "    }\n",
    "\n",
    "\n",
    "def fetch_episodes_info(serial_id, url_title, is_anime, headers, cookies, retries=3):\n",
    "    url = f'https://fancaps.net/{\"anime\" if is_anime else \"tv\"}/showimages.php?{serial_id}-{url_title}'\n",
    "    episodes_info = []\n",
    "\n",
    "    continue_search = True\n",
    "    page = 1\n",
    "\n",
    "    links_search_param = {'style': 'color:black;'} if is_anime else {'class': 'btn btn-block'}\n",
    "\n",
    "    while continue_search:\n",
    "        fetch_url = url\n",
    "        if page != 1:\n",
    "            fetch_url += '&page=' + str(page)\n",
    "\n",
    "        response_content = fetch_url_with_retries(fetch_url, headers, cookies, retries)\n",
    "\n",
    "        soup = BeautifulSoup(response_content, 'html.parser')\n",
    "        contentbody_section = soup.find('section', {'id': 'contentbody'})\n",
    "\n",
    "        target_links = contentbody_section.find_all('a', links_search_param)\n",
    "\n",
    "        if target_links:\n",
    "            page += 1\n",
    "            for a in target_links:\n",
    "                link = a.get('href')\n",
    "                episode_id = int(link.split('?', 1)[1].split('-', 1)[0])\n",
    "\n",
    "                link_text = a.text.strip()\n",
    "                marker_index = link_text.rfind('Episode ')\n",
    "\n",
    "                if marker_index == -1:\n",
    "                    if is_anime:\n",
    "                        episode_str = link_text[len('Images From '):].strip()\n",
    "                    else:\n",
    "                        episode_str = link_text[len('See More Images '):].strip()\n",
    "                else:\n",
    "                    episode_str = link_text[marker_index + len('Episode '):].strip()\n",
    "\n",
    "                episodes_info.append((episode_id, episode_str))\n",
    "        else:\n",
    "            continue_search = False\n",
    "\n",
    "    return episodes_info\n",
    "\n",
    "\n",
    "def fetch_last_image_id(url, page, is_anime, headers, cookies, retries=3):\n",
    "    to_search_url = url + '&page=' + str(page) if page != 1 else url\n",
    "    response_content = fetch_url_with_retries(to_search_url, headers, cookies, retries)\n",
    "\n",
    "    soup = BeautifulSoup(response_content, 'html.parser')\n",
    "    contentbody_section = soup.find('section', {'id': 'contentbody'})\n",
    "\n",
    "    marker = 'Episode Screencaps' if is_anime else 'Episode Images'\n",
    "    h3_marker = contentbody_section.find('h3', string=marker)\n",
    "    images_div = h3_marker.find_next('div', class_='row')\n",
    "    images = images_div.find_all('img')\n",
    "\n",
    "    if not images:\n",
    "        return\n",
    "\n",
    "    last_image = images[-1]\n",
    "    last_image_src = last_image['src']\n",
    "    last_image_id = extract_image_id(last_image_src)\n",
    "\n",
    "    return last_image_id\n",
    "\n",
    "\n",
    "def last_image_id_binary_search(url, l, r, is_anime, headers, cookies, retries=3):\n",
    "    previous_last_image_id = None\n",
    "    while l + 1 != r:\n",
    "        m = l + (r - l) // 2\n",
    "\n",
    "        last_image_id = fetch_last_image_id(url, m, is_anime, headers, cookies, retries)\n",
    "\n",
    "        if last_image_id is None:\n",
    "            r = m\n",
    "        else:\n",
    "            previous_last_image_id = last_image_id\n",
    "            l = m\n",
    "\n",
    "    if previous_last_image_id is None:\n",
    "        previous_last_image_id = fetch_last_image_id(url, l, is_anime, headers, cookies, retries)\n",
    "\n",
    "    return previous_last_image_id, l\n",
    "\n",
    "\n",
    "translation_table = str.maketrans({\n",
    "    'ō': 'o',\n",
    "    'á': 'a',\n",
    "    'ú': 'u'\n",
    "})\n",
    "\n",
    "\n",
    "def translate_string_to_url_format(text: str):\n",
    "    return re.sub(r'\\/\\/|[^a-zA-Z0-9&~%`]', '_', re.sub(r'[.!?()+\"]', '', text.translate(translation_table)))\n",
    "\n",
    "\n",
    "def is_first_char_digit(input_string: str):\n",
    "    return input_string and input_string[0].isdigit()\n",
    "\n",
    "\n",
    "def fetch_episode_images_info(episode_id, episode_str, url_title, is_anime, headers, cookies, retries=3, default_last_page=20):\n",
    "    episode_str_replaced = translate_string_to_url_format(episode_str)\n",
    "\n",
    "    if is_first_char_digit(episode_str_replaced) or (is_anime and episode_str_replaced.startswith('Special_')):\n",
    "        episode_url_param = f'Episode_{episode_str_replaced}'\n",
    "    else:\n",
    "        episode_url_param = episode_str_replaced\n",
    "\n",
    "    url = f'https://fancaps.net/{\"anime\" if is_anime else \"tv\"}/episodeimages.php?{episode_id}-{url_title}/{episode_url_param}'\n",
    "\n",
    "    response_content = fetch_url_with_retries(url, headers, cookies, retries)\n",
    "\n",
    "    soup = BeautifulSoup(response_content, 'html.parser')\n",
    "    contentbody_section = soup.find('section', {'id': 'contentbody'})\n",
    "\n",
    "    marker = 'Episode Screencaps' if is_anime else 'Episode Images'\n",
    "    h3_marker = contentbody_section.find('h3', string=marker)\n",
    "    images_div = h3_marker.find_next('div', class_='row')\n",
    "\n",
    "    first_image = images_div.find('img')\n",
    "    first_image_src = first_image['src']\n",
    "    first_image_id = extract_image_id(first_image_src)\n",
    "\n",
    "    current_page = default_last_page\n",
    "    step = 15\n",
    "\n",
    "    last_image_id = fetch_last_image_id(url, current_page, is_anime, headers, cookies, retries)\n",
    "\n",
    "    if last_image_id is None:\n",
    "        current_page -= 1\n",
    "        last_image_id = fetch_last_image_id(url, current_page, is_anime, headers, cookies, retries)\n",
    "        if last_image_id is not None:\n",
    "            return (first_image_id, last_image_id, current_page)\n",
    "\n",
    "        while last_image_id is None:\n",
    "            current_page = max(current_page - step, 1)\n",
    "            last_image_id = fetch_last_image_id(url, current_page, is_anime, headers, cookies, retries)\n",
    "\n",
    "        l = current_page\n",
    "        r = current_page + step\n",
    "    else:\n",
    "        current_page += 1\n",
    "        test_for_no_images = fetch_last_image_id(url, current_page, is_anime, headers, cookies, retries)\n",
    "        if test_for_no_images is None:\n",
    "            return (first_image_id, last_image_id, current_page - 1)\n",
    "\n",
    "        while last_image_id is not None:\n",
    "            current_page += step\n",
    "            last_image_id = fetch_last_image_id(url, current_page, is_anime, headers, cookies, retries)\n",
    "\n",
    "        l = current_page - step\n",
    "        r = current_page\n",
    "\n",
    "    last_image_id, last_image_page = last_image_id_binary_search(url, l, r, is_anime, headers, cookies, retries)\n",
    "\n",
    "    return (first_image_id, last_image_id, last_image_page)\n",
    "\n",
    "\n",
    "def fetch_tv_info(title, season, id, headers, cookies, retries=3):\n",
    "    try:\n",
    "        episodes_data = []\n",
    "        title_replaced = translate_string_to_url_format(title)\n",
    "        url_title = f'{title_replaced}_Season_{season}' if season else title_replaced\n",
    "\n",
    "        episodes_info = fetch_episodes_info(id, url_title, False, headers, cookies, retries)\n",
    "        default_last_page = 20\n",
    "\n",
    "        for episode_id, episode_str in episodes_info:\n",
    "            first_image_id, last_image_id, default_last_page = fetch_episode_images_info(episode_id, episode_str, url_title, False, headers, cookies, retries, default_last_page)\n",
    "            episodes_data.append({\n",
    "                'id': episode_id,\n",
    "                'episode': episode_str,\n",
    "                'first_image_id': first_image_id,\n",
    "                'last_image_id': last_image_id\n",
    "            })\n",
    "\n",
    "        return {\n",
    "            'id': id,\n",
    "            'title': title,\n",
    "            'season': season,\n",
    "            'episodes': episodes_data\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f'Exception while fetching tv: {title} Season {season} ({id}). {e}')\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def fetch_anime_info(title, alternative_title, id, headers, cookies, retries=3):\n",
    "    try:\n",
    "        episodes_data = []\n",
    "        url_title = translate_string_to_url_format(title)\n",
    "\n",
    "        episodes_info = fetch_episodes_info(id, url_title, True, headers, cookies, retries)\n",
    "        default_last_page = 20\n",
    "\n",
    "        for episode_id, episode_str in episodes_info:\n",
    "            first_image_id, last_image_id, default_last_page = fetch_episode_images_info(episode_id, episode_str, url_title, True, headers, cookies, retries, default_last_page)\n",
    "            episodes_data.append({\n",
    "                'id': episode_id,\n",
    "                'episode': episode_str,\n",
    "                'first_image_id': first_image_id,\n",
    "                'last_image_id': last_image_id\n",
    "            })\n",
    "\n",
    "        return {\n",
    "            'id': id,\n",
    "            'title': title,\n",
    "            'alternative_title': alternative_title,\n",
    "            'episodes': episodes_data\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f'Exception while fetching anime: {title} ({id}). {e}')\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_movies(movies_list, headers, cookies, retries = 3):\n",
    "    movies_data = []\n",
    "    movies_with_error = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        with tqdm(total=len(movies_list)) as pbar:\n",
    "            future_to_movie = {executor.submit(fetch_movie_with_retries, title, year, movie_id, headers, cookies, retries): (title, movie_id) for title, year, movie_id in movies_list}\n",
    "            for future in concurrent.futures.as_completed(future_to_movie):\n",
    "                title, movie_id = future_to_movie[future]\n",
    "                try:\n",
    "                    movie_data = future.result()\n",
    "                    if movie_data is None:\n",
    "                        movies_with_error.append(movie_id)\n",
    "                    else:\n",
    "                        movies_data.append(movie_data)\n",
    "                except Exception as e:\n",
    "                    movies_with_error.append(movie_id)\n",
    "                    print(f'Exception for movie: {title} ({movie_id}). Error: {e}')\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    movies_data.sort(key=lambda x: x['title'])\n",
    "\n",
    "    return movies_data, movies_with_error\n",
    "\n",
    "\n",
    "def parse_tv(tv_list, headers, cookies, retries = 3):\n",
    "    tv_data = []\n",
    "    tv_with_error = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        with tqdm(total=len(tv_list)) as pbar:\n",
    "            future_to_tv = {executor.submit(fetch_tv_info, title, season, tv_id, headers, cookies, retries): (title, season, tv_id) for title, season, tv_id in tv_list}\n",
    "            for future in concurrent.futures.as_completed(future_to_tv):\n",
    "                title, season, tv_id = future_to_tv[future]\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                    if data is None:\n",
    "                        tv_with_error.append(tv_id)\n",
    "                    else:\n",
    "                        tv_data.append(data)\n",
    "                except Exception as e:\n",
    "                    tv_with_error.append(tv_id)\n",
    "                    print(f'Exception for tv: {title} Season {season} ({tv_id}). {e}')\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    tv_data.sort(key=lambda x: x['title'])\n",
    "\n",
    "    return tv_data, tv_with_error\n",
    "\n",
    "\n",
    "def parse_anime(anime_list, headers, cookies, retries = 3):\n",
    "    anime_data = []\n",
    "    anime_with_error = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        with tqdm(total=len(anime_list)) as pbar:\n",
    "            future_to_anime = {executor.submit(fetch_anime_info, title, alt_title, anime_id, headers, cookies, retries): (title, anime_id) for title, alt_title, anime_id in anime_list}\n",
    "            for future in concurrent.futures.as_completed(future_to_anime):\n",
    "                title, anime_id = future_to_anime[future]\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                    if data is None:\n",
    "                        anime_with_error.append(anime_id)\n",
    "                    else:\n",
    "                        anime_data.append(data)\n",
    "                except Exception as e:\n",
    "                    anime_with_error.append(anime_id)\n",
    "                    print(f'Exception for anime: {title} ({anime_id}). {e}')\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    anime_data.sort(key=lambda x: x['title'])\n",
    "\n",
    "    return anime_data, anime_with_error\n",
    "\n",
    "\n",
    "def fetch_movie_images_ids(movie_id, headers, cookies, retries = 3):\n",
    "    page = 1\n",
    "    images_ids = []\n",
    "    continue_search = True\n",
    "    base_url = f'https://fancaps.net/movies/MovieImages.php?movieid={movie_id}&page='\n",
    "\n",
    "    while continue_search:\n",
    "        fetch_url = base_url + str(page)\n",
    "        responce_content = fetch_url_with_retries(fetch_url, headers, cookies, retries)\n",
    "        soup = BeautifulSoup(responce_content, 'html.parser')\n",
    "        images_bar = soup.find('section', {'id': 'contentbody'}).find('div', {'class': 'middle_bar'})\n",
    "\n",
    "        title = images_bar.find('h2', {'class': 'post_title'})\n",
    "        images_div = title.find_next_sibling('div', {'class': ''})\n",
    "        next_page_url = images_bar.select_one('ul.pagination li:last-child a').get('href')\n",
    "\n",
    "        if images_div:\n",
    "            page += 1\n",
    "            found_images = images_div.find_all('img', {'class': 'imageFade'})\n",
    "            continue_search = found_images and len(found_images) > 0 and next_page_url != '#'\n",
    "            for found_image in found_images:\n",
    "                images_ids.append(extract_image_id(found_image.get('src')))\n",
    "        else:\n",
    "            continue_search = False\n",
    "\n",
    "    return images_ids\n",
    "\n",
    "\n",
    "def fetch_episode_images_ids(url_title, episode_data, is_anime, headers, cookies, retries = 3):\n",
    "    page = 1\n",
    "    images_ids = []\n",
    "\n",
    "    episode_id = episode_data['id']\n",
    "    episode_str = episode_data['episode']\n",
    "\n",
    "    episode_str_replaced = translate_string_to_url_format(episode_str)\n",
    "    if is_first_char_digit(episode_str_replaced) or (is_anime and episode_str_replaced.startswith('Special_')):\n",
    "        episode_url_param = f'Episode_{episode_str_replaced}'\n",
    "    else:\n",
    "        episode_url_param = episode_str_replaced\n",
    "\n",
    "    url = f'https://fancaps.net/{\"anime\" if is_anime else \"tv\"}/episodeimages.php?{episode_id}-{url_title}/{episode_url_param}'\n",
    "\n",
    "    while True:\n",
    "        fetch_url = url\n",
    "        if page != 1:\n",
    "            fetch_url += '&page=' + str(page)\n",
    "\n",
    "        response_content = fetch_url_with_retries(fetch_url, headers, cookies, retries)\n",
    "\n",
    "        soup = BeautifulSoup(response_content, 'html.parser')\n",
    "        contentbody_section = soup.find('section', {'id': 'contentbody'})\n",
    "\n",
    "        marker = 'Episode Screencaps' if is_anime else 'Episode Images'\n",
    "        h3_marker = contentbody_section.find('h3', string=marker)\n",
    "        images_div = h3_marker.find_next('div', class_='row')\n",
    "        images = images_div.find_all('img')\n",
    "\n",
    "        if not images:\n",
    "            break\n",
    "\n",
    "        for image in images:\n",
    "            image_src = image['src']\n",
    "            image_id = extract_image_id(image_src)\n",
    "            images_ids.append(image_id)\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    return images_ids\n",
    "\n",
    "\n",
    "def fix_movies_conflicts(movies_data: list, headers, cookies, retries = 3):\n",
    "    if len(movies_data) <= 1:\n",
    "        return 0, 0\n",
    "\n",
    "    movies_conflict_indices = set()\n",
    "    movies_data.sort(key=lambda x: x['first_image_id'])\n",
    "\n",
    "    movies_iterator = iter(movies_data)\n",
    "    current_movie = next(movies_iterator)\n",
    "    index_counter = 0\n",
    "\n",
    "    for movie in movies_iterator:\n",
    "        last_movie = current_movie\n",
    "        current_movie = movie\n",
    "\n",
    "        start_current = current_movie['first_image_id']\n",
    "        end_last = last_movie['last_image_id']\n",
    "\n",
    "        if start_current <= end_last:\n",
    "            movies_conflict_indices.add(index_counter)\n",
    "            movies_conflict_indices.add(index_counter + 1)\n",
    "\n",
    "        index_counter += 1\n",
    "\n",
    "    movies_to_fix = [movies_data[i] for i in movies_conflict_indices]\n",
    "    errors_while_fix_movies = set()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        with tqdm(total=len(movies_conflict_indices)) as pbar:\n",
    "            future_get_images_id = {executor.submit(fetch_movie_images_ids, movie_data['id'], headers, cookies, retries): movie_data for movie_data in movies_to_fix}\n",
    "            for future in concurrent.futures.as_completed(future_get_images_id):\n",
    "                movie_data = future_get_images_id[future]\n",
    "                movie_id = movie_data['id']\n",
    "                try:\n",
    "                    images_ids = future.result()\n",
    "                    movie_data['valid_images_ids'] = images_ids\n",
    "                except Exception as e:\n",
    "                    errors_while_fix_movies.add(movie_id)\n",
    "                    print(f'Exception for movie: {movie_data[\"title\"]} ({movie_id}). Error: {e}')\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    total_errors = len(errors_while_fix_movies)\n",
    "    if total_errors > 0:\n",
    "        movies_data = [movie_data for movie_data in movies_data if movies_data['id'] not in errors_while_fix_movies]\n",
    "\n",
    "    movies_data.sort(key=lambda x: x['title'])\n",
    "\n",
    "    return len(movies_to_fix) - total_errors, total_errors\n",
    "\n",
    "\n",
    "def auto_title_url_formatter(item: dict):\n",
    "    title = translate_string_to_url_format(item['title'])\n",
    "    season = item.get('season')\n",
    "    if season:\n",
    "        return f'{title}_Season_{season}'\n",
    "\n",
    "    return title\n",
    "\n",
    "\n",
    "def fix_tv_and_anime_conflicts(data: list, is_anime, headers, cookies, retries = 3):\n",
    "    images_intervals = []\n",
    "    for i, item in enumerate(data):\n",
    "        episodes = item['episodes']\n",
    "        for j, episode in enumerate(episodes):\n",
    "            first_image_id = episode['first_image_id']\n",
    "            last_image_id = episode['last_image_id']\n",
    "            images_intervals.append((first_image_id, last_image_id, i, j))\n",
    "\n",
    "    if len(images_intervals) <= 1:\n",
    "        return 0, 0\n",
    "\n",
    "    intervals_conflict_indices = []\n",
    "    images_intervals.sort(key=lambda x: x[0])\n",
    "\n",
    "    intervals_iterator = iter(images_intervals)\n",
    "    current_interval = next(intervals_iterator)\n",
    "\n",
    "    for interval in intervals_iterator:\n",
    "        last_interval = current_interval\n",
    "        current_interval = interval\n",
    "\n",
    "        start_current = current_interval[0]\n",
    "        end_last = last_interval[1]\n",
    "\n",
    "        if start_current <= end_last:\n",
    "            intervals_conflict_indices.append((last_interval[2], last_interval[3]))\n",
    "            intervals_conflict_indices.append((current_interval[2], current_interval[3]))\n",
    "\n",
    "    del images_intervals\n",
    "\n",
    "    episodes_to_fix = [(i, j, auto_title_url_formatter(item), item['episodes'][j]) for i, j in intervals_conflict_indices for item in [data[i]]]\n",
    "\n",
    "    del intervals_conflict_indices\n",
    "\n",
    "    errors_while_fix = dict()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        with tqdm(total=len(episodes_to_fix)) as pbar:\n",
    "            future_get_images_id = {executor.submit(fetch_episode_images_ids, title, episode, is_anime, headers, cookies, retries): (i, j, title, episode) for i, j, title, episode in episodes_to_fix}\n",
    "            for future in concurrent.futures.as_completed(future_get_images_id):\n",
    "                i, j, title, episode_data = future_get_images_id[future]\n",
    "                episode_id = episode_data['id']\n",
    "                try:\n",
    "                    images_ids = future.result()\n",
    "                    episode_data['valid_images_ids'] = images_ids\n",
    "                except Exception as e:\n",
    "                    item_with_errors = errors_while_fix.get(i)\n",
    "\n",
    "                    if item_with_errors is None:\n",
    "                        errors_while_fix[i] = set([j])\n",
    "                    else:\n",
    "                        item_with_errors.add(j)\n",
    "\n",
    "                    print(f'Exception for: {title} ({episode_id}). Error: {e}')\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    total_errors = len(errors_while_fix)\n",
    "\n",
    "    for i, error_items in errors_while_fix.items():\n",
    "        episodes = data[i]['episodes']\n",
    "        episodes = [episode for idx, episode in enumerate(episodes) if idx not in error_items]\n",
    "\n",
    "    return len(episodes_to_fix) - total_errors, total_errors\n",
    "\n",
    "\n",
    "def dump_json(data, filepath: str):\n",
    "    with open(filepath, 'w') as file:\n",
    "        json.dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse movies data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "movies_data, movies_with_error = parse_movies(movies_list, headers, cookies, retries = 3)\n",
    "\n",
    "print('=' * 30)\n",
    "print('Total movies parsed:', len(movies_data))\n",
    "print('Total errors:', len(movies_with_error))\n",
    "print('Total time:', round(time.time() - start_time), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "total_fixed, total_errors = fix_movies_conflicts(movies_data, headers, cookies, retries = 3)\n",
    "\n",
    "print('=' * 30)\n",
    "print('Total interval conflicts fixed:', total_fixed)\n",
    "print('Unable to fix:', total_errors)\n",
    "print('Total time:', round(time.time() - start_time), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_json(movies_data, movies_data_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse tv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "tv_data, tv_with_error = parse_tv(tv_list, headers, cookies, retries = 5)\n",
    "\n",
    "print('=' * 30)\n",
    "print('Total tv parsed:', len(tv_data))\n",
    "print('Total errors:', len(tv_with_error))\n",
    "print('Total time:', round(time.time() - start_time), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "total_fixed, total_errors = fix_tv_and_anime_conflicts(tv_data, False, headers, cookies, retries = 5)\n",
    "\n",
    "print('=' * 30)\n",
    "print('Total interval conflicts fixed:', total_fixed)\n",
    "print('Unable to fix:', total_errors)\n",
    "print('Total time:', round(time.time() - start_time), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_json(tv_data, tv_data_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse anime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "anime_data, anime_with_error = parse_anime(anime_list, headers, cookies, retries = 5)\n",
    "\n",
    "print('=' * 30)\n",
    "print('Total anime parsed:', len(anime_data))\n",
    "print('Total errors:', len(anime_with_error))\n",
    "print('Total time:', round(time.time() - start_time), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "total_fixed, total_errors = fix_tv_and_anime_conflicts(anime_data, True, headers, cookies, retries = 5)\n",
    "\n",
    "print('=' * 30)\n",
    "print('Total interval conflicts fixed:', total_fixed)\n",
    "print('Unable to fix:', total_errors)\n",
    "print('Total time:', round(time.time() - start_time), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_json(anime_data, anime_data_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(movies_data_filepath, 'r') as file:\n",
    "    movies_data_json = json.load(file)\n",
    "\n",
    "movies_data = list(movies_data_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tv_data_filepath, 'r') as file:\n",
    "    tv_data_json = json.load(file)\n",
    "\n",
    "tv_data = list(tv_data_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(anime_data_filepath, 'r') as file:\n",
    "    anime_data_json = json.load(file)\n",
    "\n",
    "anime_data = list(anime_data_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
